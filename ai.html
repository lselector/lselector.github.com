<html>
<head>
   <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
   <meta name="Description" content="Description" content="Hadoop noSQL">
   <meta name="GENERATOR" content="Mozilla/4.7 [en] (WinNT; U) [Netscape]">
   <meta name="Author" content="Lev Selector">
   <title>Data Science, Machine Learning & Artificial Intelligence</title>
<!--

"Description" content="Resume, Data Science, Machine Learning & Artificial Intelligence, Hadoop, Hive, HBase, Cassandra, Cloudera, Scala, Spark, SparkSQL, Python, Perl, Actionscript, JavaScript, Sybase, Oracle, Java, Servlet, JSP, XML, XSL, Wireless, Palm, Online Trading, Financial Applications, C/C++, Unix, Solaris, Linux, Sybase, Oracle, DB2, MySQL, CGI, Apache, mod_perl, HTML, DHTML, CSS, SSL, WebLogic, Apache, Dreamweaver, Adobe Photoshop, Algorithmic trading, Financial Databases"

-->
<link REL="stylesheet" TYPE="text/css" HREF="style0.css" >
<style type="text/css">
<!--
.style1 {
	color: #CC0000;
	font-weight: bold;
}
.code {color: #0000FF}
.comment {color: #669900}
-->
</style>
</head>
<body text="#000000" bgcolor="#FFFFFF">
<a NAME="top"></a>
<table BORDER=0 CELLSPACING=0 CELLPADDING=0 WIDTH="100%" >
  <tr>
    <td ALIGN=LEFT WIDTH="1%"><spacer type=block width=1 height=1></td>
    <td ALIGN=LEFT><b><font color="#CC0000">LevSelector.com</font></b>
      <spacer type=block width=1 height=1></td>
    <td ALIGN=RIGHT VALIGN=BOTTOM><spacer type=block width=1 height=1>
      <b><font color="#CC0000">New
        York</font></b></td>
    <td ALIGN=LEFT WIDTH="1%"><spacer type=block width=1 height=1></td>
  </tr>
</table>
<table BORDER=0 CELLSPACING=0 CELLPADDING=0 WIDTH="100%" >
  <tr>
    <td><spacer type=block width=1 height=1></td>
  </tr>
  <tr>
    <td BGCOLOR="#66CCFF"><spacer type=block width=1 height=1></td>
  </tr>
  <tr>
    <td><spacer type=block width=1 height=1></td>
  </tr>
</table>
<table BORDER=0 CELLSPACING=0 CELLPADDING=0 WIDTH="100%" BGCOLOR="#C4ECFF" >
  <tr>
    <td ALIGN=LEFT WIDTH="1%"><spacer type=block width=1 height=1></td>
    <td ALIGN=LEFT><spacer type=block width=1 height=1>
      <b><a href="index.html">home</a> > Data Science, Machine Learning &amp; Artificial Intelligence</b></td>
    <td ALIGN=RIGHT VALIGN=BOTTOM><spacer type=block width=1 height=1>
      <b></b></td>
    <td ALIGN=LEFT WIDTH="1%"><spacer type=block width=1 height=1></td>
  </tr>
</table>
<table BORDER=0 CELLSPACING=0 CELLPADDING=0 WIDTH="100%" >
  <tr>
    <td><spacer type=block width=1 height=1></td>
  </tr>
  <tr>
    <td BGCOLOR="#51C7FF"><spacer type=block width=1 height=1></td>
  </tr>
  <tr>
    <td><spacer type=block width=1 height=1></td>
  </tr>
</table>
<p><b><font color="#CC0000">Data Science, Machine Learning &amp; Artificial Intelligence</font></b>
<table width="95%" border="0" cellpadding="3" cellspacing="0">
  <tr>
    <td class="style1">On This Page</td>
    <td class="style1">More</td>
    <td class="style1">Other Pages</td>
  </tr>
  <tr>
    <td valign="top"><p> - <a href="#intro">intro</a> - <br>
      - x - <br>
      - x -</p></td>
    <td valign="top">-<br></td>
    <td valign="top"><p> -<br>
      <br>
    </p></td>
  </tr>
</table>
<p class="sectionheader"><a NAME="intro"></a>Intro ------------------------------</p>
<pre>How to learn "Machine Learning" and "Artificial Intelligence"
updated September 10, 2018

All my slides are here:
   <a href="https://goo.gl/3v8DAS">https://goo.gl/3v8DAS</a>

I have uploaded some stuff here:
   <a href="http://myhash.com/ai/" target="_blank">http://myhash.com/ai/</a><a href="http://myhash.com/ai/ai.html" target="_blank"></a>

===========================================
Unix:
You need basic working knowledge of Unix / Linux.
 - buy yourself a Macbook and learn to work from terminal.
 - book - Learning the UNIX Operating System O'Reilly
 - tutorials on youtube
     grep, find, set vs env, running script vs sourcing script 
 - vi editor:
   - <a href="http://www.levselector.com/vi.html" target="_blank">http://www.levselector.com/vi.html</a>
   - tutorials on youtube

===========================================
Python:
  anaconda: <a href="https://www.anaconda.com/download/" target="_blank">https://www.anaconda.com/download/</a>
  ipython, 
  ipython notebook (Jupyter)
  python versions 2.7 vs 3.x
  numpy, pandas DataFrame, pd.read_csv(file), df.to_csv(file)
===========================================
Math: 
 - calculus: 
     e = 2.71828... lim( (1+1/n)**n ) for large n
     derivatives and integrals, 
     Dirac Delta Function,
     multivariate calculus, partial derivatives, 
     gradient,
 - linear algebra: 
     vector, matrix, dot-product of vectors and matrices,
     vector spaces, base vectors, matrices of space transformations,
     determinant, rank, 
     eigen vectors, eigen values, 
     inverse matrix, 
     tensor
===========================================
Probability &amp; Statistics:

 - probability: 
     definition, 
     combinatorics, subsets,  C(n,m) = n!/(m!*(n-m)!)
     Venn Diagrams, 
     Conditional Probability, Bayes Theorem,
     random variable, probability distribution, expected value, 
     variance, standard deviation,
     discrete and continous cases,
     PDF (Probability Density Function) vs Cumulative Probability Function ( CPF),
     Binomial Distribution, 
     Poisson and Exponential Distributions, 
     Uniform Distribution,
     Central Limit Theorem and Normal (Gaussian) Distribution
     exp(-x^2/2)

 - statistics: 
     Sample mean (average), Sample Standard Deviation (why /(N-1) ?), Median 
     Linear Regression - draw line through points, 
     OLS = Ordinary Least Squares (minimizing quadratic error), 
     R2  = Coefficient of Determination

     Confidence interval, z-score
       not needed: Student&rsquo;s t-distribution, Chi-squared distribution, F-distribution, Gamma distribution
     Stochastic Processes, Time Series Analysis, 
     Random Walk, Brownian motion, Diffusion,
     Poisson Process/distribution, exponential distribution
     white noise, gaussian noise
     Markov Process
     Monte Carlo method, MCMC (Markov Chain Monte Carlo)
     Correlation function, Autocorrelation
     Fourier Analysis, filtering to reduce noise
     Extracting Signal from Noise by synchronization (S/N improves ~sqrt(N)),

===========================================
<strong>You need to learn the meaning of these words:</strong>
</pre>
<table width="90%" border="2" cellspacing="0" cellpadding="2">
  <tr bgcolor="#FFFF99">
    <td> <pre>Data Science (DS) - use computer to process data from different sourceess
     (CSV files, Databases), apply statistics, make graphs </pre></td>
  </tr>
  <tr bgcolor="#FFCC00">
    <td> <pre>Machine Learning (ML) - subset of DS to extract patterns from data
                        to do predictions. </pre>
    <pre>  ML Example - Linear Regression (draw streight line through points)

     Input data array of points data = [(x0,y0), (x1,y1), ...]
     model function                                
          def linear_regression(x, [a, b]):                                  
              y = a*x + b
              return y

     training of the model: find two numbers (slope a and intercept b)
                            which do best fit between the model and data.
                            (minimize the error)

  ML - many types of models (linear regression and logistic regression,
       support vector machines, K-nearest neighbors, K-means clustering,
       decision trees (RandomForest, XGBoost, etc.), Neural Networks, etc.) </pre></td>
  </tr>
  <tr bgcolor="#00FFCC">
    <td> <pre>Deep Learning (DL) - ML implemented using multi-layered structures (Networks)</pre></td>
  </tr>
  <tr bgcolor="#FFFFFF">
    <td bgcolor="#FFCCCC"> <pre>Artificial Intelligence (AI) - DL algorithm trained to perform function
  which are usually associated only with humans 
  (vision, speech comprehension, autonomous driving, etc.) </pre></td>
  </tr>
</table>
<pre>
     data cleaning, scaling, normalization
     synthetic data augmentation
     highly-unbalanced data (minority &amp; majority class, imputing data in minority class)
       
     sparse matrix, sparse matrix data representation (Yale format)
      - <a href="https://en.wikipedia.org/wiki/Sparse_matrix" target="_blank">https://en.wikipedia.org/wiki/Sparse_matrix</a>

     feature engineering (extraction/preparation)</pre>
<table width="90%" border="2" cellspacing="0" cellpadding="2">
  <tr bgcolor="#CCFFFF">
    <td><pre>Dimensional Reduction, 
</pre></td>
    <td>&nbsp;</td>
  </tr>
  <tr bgcolor="#CCFFFF">
    <td>PCA (Principal Components Analysis)</td>
    <td>&nbsp;</td>
  </tr>
  <tr bgcolor="#CCFFFF">
    <td>LDA (Linear Discriminant Analysis) </td>
    <td>statistical method to find a linear combination of features to achieve separation of two or more classes. 
  Used for dimensionality reduction before classification. Similar to PCA (Principal Component Analysis).<br>
  Note: LDA also stands for Latent Dirichlet Allocation - a generative probabilistic model (to find topics in texts).</td>
  </tr>
  <tr bgcolor="#CCFFFF">
    <td>Regression</td>
    <td>regress vs proress, simplify (for example, from 100 (x,y) pairs to 2 numbers (slope, intercept))</td>
  </tr>
  <tr bgcolor="#CCFFFF">
    <td>Linear regression</td>
    <td>OLS = Ordinary Least Squares</td>
  </tr>
  <tr bgcolor="#CCFFFF">
    <td>Logistic regression</td>
    <td><p>classification
         : 
      1 var to 1 binary, multi-var to 1 binary,or 
    multi-var to several classes (multinomial)<br>
    We model log-likelihood 
      as a linear combination of some predictors:<br>
      &nbsp;  
    logit(p) = log(p/(1-p))  = b0 + b1*x1 + b2*x2 + . . . + bN*xN</p></td>
  </tr>
  <tr bgcolor="#CCFFFF">
    <td>Propensity</td>
    <td>&nbsp;</td>
  </tr>
  <tr bgcolor="#CCFFFF">
    <td>Bayesian theorem / approach</td>
    <td>&nbsp;</td>
  </tr>
  <tr bgcolor="#CCFFFF">
    <td>K Nearest Neighbors (KNN) - tuning "K"</td>
    <td>&nbsp;</td>
  </tr>
  <tr bgcolor="#CCFFFF">
    <td>K-means clustering</td>
    <td>&nbsp;</td>
  </tr>
  <tr bgcolor="#CCFFFF">
    <td>SVM = Support Vector Machines</td>
    <td>&nbsp;</td>
  </tr>
  <tr bgcolor="#CCFFFF">
    <td>Decision Tree</td>
    <td>Ensemble Methods, bagging, boosting, Random Forest, XGBoost</td>
  </tr>
</table>
<pre>
     softmax

     training and test data, <strong>overfitting</strong>
     Regularization (ridge regression, LASSO)
       dropout, adding noise

     bias-variance tradeoff
     Determining feature importance

     Supervised vs unsupervised Machine Learning

     Stochastic Gradient Descent</pre>
<table width="90%" border="2" cellspacing="0" cellpadding="2">
  <tr bgcolor="#CCFFFF">
    <td><pre>NLP = Natural Language Processing
sentiment analysis
text as a 'bag of words'<br>
TF-IDF = Term frequency&ndash;inverse document frequency</pre></td>
  </tr>
</table>
<p>---</p>
<table width="90%" border="2" cellspacing="0" cellpadding="2">
  <tr bgcolor="#CCFFFF">
    <td><pre>Classifier
Outlier/anomaly Detection

     ROC curve = Receiver Operating Characteristic curve
        True Positive Rate vs False Positive Rate (TPR vs FPR)
     Precision P = 1-FPR
     Recall = TPR
     F1 score = 2/(1/TPR + 1/P)   

     confusion matrix = actual (0,1) vs predicted (0,1)

    ------------+--------------+---------------
                | predicted No | predicted Yes
    ------------+--------------+---------------
    Actual No   | TN=50        |  FP=10    
    Actual Yes  | FN=5         |  TP=100
    ------------+--------------+---------------

    where
    TP, TN, FP, FN = True/False Positive/Negative</pre></td>
  </tr>
</table>
<pre>

ML/DL libraries and tools:
     Scikit-learn (sklearn) - <a href="http://scikit-learn.org/stable/" target="_blank">http://scikit-learn.org/stable/</a>
     TF = TensorFlow - <a href="https://www.tensorflow.org/" target="_blank">https://www.tensorflow.org/</a>
     PyTorch - <a href="https://pytorch.org/" target="_blank">https://pytorch.org/</a>
     Keras - <a href="https://keras.io/" target="_blank">https://keras.io/</a>
     XGBoost - <a href="https://xgboost.ai/" target="_blank">https://xgboost.ai/</a> 
     MXNet (Apache DL library) - <a href="https://mxnet.apache.org/" target="_blank">https://mxnet.apache.org/</a> 
     fastText - <a href="https://fasttext.cc/" target="_blank">https://fasttext.cc/</a>
     NLTK - Natural Language Toolkit - <a href="https://www.nltk.org/" target="_blank">https://www.nltk.org/</a>
     CNTK = Microsoft Cognitive Toolkit - <a href="https://cntk.ai/" target="_blank">https://cntk.ai/</a> 
     H2O - <a href="https://www.h2o.ai/" target="_blank">https://www.h2o.ai/</a> - parallel training and execution on cloud
     SageMaker - <a href="https://aws.amazon.com/sagemaker/" target="_blank">https://aws.amazon.com/sagemaker/</a> - on Amazon Cloud
     Google AI - <a href="https://cloud.google.com/products/ai/" target="_blank">https://cloud.google.com/products/ai/</a> - Cloud AutoML, Cloud Machine Learning Engine, Cloud TPUs, BigQuery ML, ...
     Nicrosoft Azure ML - <a href="https://azure.microsoft.com/en-us/overview/machine-learning/" target="_blank">https://azure.microsoft.com/en-us/overview/machine-learning/</a> - 

---
     Daily time-series - detecting seasonality with Fourier Transforms
     removing trend
     Moving-Window Averages

     Recommendation Engine
     Collaborative Filtering

     Neural Networks (NN)
     perceptron, multilayer perceptron   
     The XOR problem, hidden layers, non-linearity, ReLU
     Feed-Forward Network
     nodes, connections, weights, biases, activations

     learning as optimization problem
     cost function, objective function, loss function, regret
     back-propagation, SGD (Stochastic Gradient Descent)

     Boltzman Machine
     RBM (Restricted Boltzman Machine, 2006), 
     Deep Belief Network</pre>
<table width="90%" border="2" cellspacing="0" cellpadding="2">
  <tr bgcolor="#FFFFCC">
    <td><pre>CNN - Convolutional Neural Network
        layers: convolutional, pooling, fully-connected
     Short overview and comparison of CNNs:
      - <a href="https://medium.com/@sidereal/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5" target="_blank">https://medium.com/@sidereal/cnns-architectures-lenet-alexnet-vgg-googlenet-resnet-and-more-666091488df5</a>

 - LeNet-5 (1998 - by Yann LeCun, 60K parameters), 
       MNIST database - handwritten digits&nbsp;(60K training, 10K testing) 
       ImageNet - 14 Mln images, 1000 classes
 - AlexNet (2012 - Alex Krizhevsky, Geoffrey Hinton, and Ilya Sutskever, 60 Mln parameters)
 - VGGNet (2014, 138 Mln parameters)
 - GoogLeNet (2014, inception blocks, 19 layers, 4 Mln parameters)
 - ResNet (2015 - Residual Neural Network, 152 layers, 25 Mln parameters, Microsoft Research)
 - Google AutoML, NASNet architecture (2017 - <a href="https://ai.googleblog.com/2017/11/automl-for-large-scale-image.html" target="_blank">https://ai.googleblog.com/2017/11/automl-for-large-scale-image.html</a> ) 

YOLO - You Look Only Once - real time object detection
Image segmentation (semantic segmentation)
U-Net: Convolutional Networks for Biomedical Image Segmentation
</pre></td>
  </tr>
  <tr bgcolor="#FFFFCC">
    <td><pre>AutoEncoder
Variational AutoEncoder

seq2seq<br>word2vec, embeddings  (king - man + woman ~= queen)<br></pre></td>
  </tr>
  <tr bgcolor="#FFFFCC">
    <td><pre>RNN - Recurrent NN<br>     BRNN - Bi-directional RNN
     Exploding/Vanishing Gradient problem
     LSTM - Long Short Term Memory (1997 -  Sepp Hochreiter and J&uuml;rgen Schmidhuber)
     GRU - Gated Recurrent Unit (2014, Univ. of Montreal, Canada) - simpler than LSTM
     Attention (2014) - <a href="https://arxiv.org/abs/1409.0473" target="_blank">https://arxiv.org/abs/1409.0473</a>
     Atention Is All You Need (2017, Transformer) - <a href="https://arxiv.org/abs/1706.03762" target="_blank">https://arxiv.org/abs/1706.03762</a>
       Read this:
         - <a href="https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/" target="_blank">https://machinelearningmastery.com/how-does-attention-work-in-encoder-decoder-recurrent-neural-networks/</a> 

Google Translate:
 - <a href="https://arxiv.org/pdf/1609.08144v2.pdf" target="_blank">https://arxiv.org/pdf/1609.08144v2.pdf</a> - 31 authors<br>   deep LSTM network <br>   8 encoder and 8 decoder layers<br>   parallelism - decreasing training time<br>   using attention and residual connections<br>   attention mechanism - connects the bottom layer of the decoder to the top layer of the encoder<br>   low-precision arithmetic for inference computations

Chatbots:
  (Natural Language Comprehension =&gt; Understanding Intent =&gt; Action)
  Amazon Connect + Amazon Lex 
  Google Dialog Flow
</pre></td>
  </tr>
</table>
<pre>

     regularization - overfitting & dropout
     Changing learing rate dynamically , ADAM optimizer
     batch-processing, minibatch, batch-size, SGD

     Cross Entropy 
     Softmax classification (uses same formula, but different meaning)
     KL-divergence (Kullback&ndash;Leibler divergence, also called relative entropy) 

     GANs (Generative Adversarial Networks)
     Generative Adversarial Examples


</pre>
<table width="90%" border="2" cellspacing="0" cellpadding="2">
  <tr bgcolor="#CCFFFF">
    <td><pre>Reinforcement Learning, Deep Reinforcement Learning
       Agent, Policy, Reward, Regret

AlphaGo, DeepMind

multi-armed bandit problem - a fixed limited set of resources 
       must be allocated between competing (alternative) choices 
       in a way that maximizes their expected gain, 
       when each choice's properties are only partially known 
       at the time of allocation, and may become better understood 
       as time passes or by allocating resources to the choice.
       The name comes from imagining a gambler at a row of 
       <strong>slot machines</strong> (sometimes known as &quot;<strong>one-armed bandits</strong>&quot;).</pre></td>
  </tr>
</table>
<pre>


===========================================

Good courses about Deep Learning (DL) and ML:
 - <a href="https://www.coursera.org/specializations/deep-learning" target="_blank">https://www.coursera.org/specializations/deep-learning</a>
 - <a href="https://www.coursera.org/specializations/machine-learning-tensorflow-gcp" target="_blank">https://www.coursera.org/specializations/machine-learning-tensorflow-gcp</a>
 - <a href="http://www.fast.ai" target="_blank">http://www.fast.ai</a>
 - <a href="https://www.udacity.com/course/deep-learning--ud730" target="_blank">https://www.udacity.com/course/deep-learning--ud730</a>
 etc.

About Coursera:
   - <a href="https://www.ted.com/talks/daphne_koller_what_we_re_learning_from_online_education" target="_blank">https://www.ted.com/talks/daphne_koller_what_we_re_learning_from_online_education
</a>
================================================================
deeplearning.ai - 5 courses - all videos on youtube:

courses 1,2,3 of 5 - 98 videos:
 - <a href="https://www.youtube.com/watch?v=7PiK4wtfvbA&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K" target="_blank">https://www.youtube.com/watch?v=7PiK4wtfvbA&list=PLBAGcD3siRDguyYYzhVwZ3tLvOyyG5k6K</a>
  course 1: video 1-41
  course 2: video 41-70
  course 3: video 71-98

course 4 of 5 - 43 videos
Convolutional Neural Network (CNN)
 - <a href="https://www.youtube.com/watch?v=Z91YCMvxdo0&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud" target="_blank">https://www.youtube.com/watch?v=Z91YCMvxdo0&list=PLBAGcD3siRDjBU8sKRk0zX9pMz9qeVxud
</a>
course 5 of 5 - 33 videos
Recurrent Neural Networks (RNN)
 - <a href="https://www.youtube.com/watch?v=5Vl-bK7tfD8&list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7" target="_blank">https://www.youtube.com/watch?v=5Vl-bK7tfD8&list=PLBAGcD3siRDittPwQDGIIAWkjz-RucAc7
</a>
fast.ai videos on youtube (parts 1,2 - 14 videos):
 - <a href="https://www.youtube.com/watch?v=IPBSB1HLNLo&list=PLCdvEQLhYkYmKTKWTrH7bHtQ1CsKZaQBl" target="_blank">https://www.youtube.com/watch?v=IPBSB1HLNLo&amp;list=PLCdvEQLhYkYmKTKWTrH7bHtQ1CsKZaQBl</a>

================================================================
You can also read this book:
   - <a href="http://www.deeplearningbook.org" target="_blank">http://www.deeplearningbook.org</a>
or find youtube videos where people discuss chapters of this book.
The book is comprehensive - but difficult to read.
You will need to do a lot of internet browsing to clarify things.

Audio interviews:
   (TWiML&AI) podcast
   - <a href="https://twimlai.com" target="_blank">https://twimlai.com</a> -

For Russian speaking - good channel on Youtube
 - <a href="https://www.youtube.com/watch?v=MYp3OwkiJAs" target="_blank">https://www.youtube.com/watch?v=MYp3OwkiJAs
</a> - <a href="https://www.youtube.com/channel/UCQj_dwbIydi588xrfjWSL5g/videos" target="_blank">https://www.youtube.com/channel/UCQj_dwbIydi588xrfjWSL5g/videos</a>

Google's Tensor Processing Units (TPUs):
 - <a href="https://www.wired.com/2017/05/google-rattles-tech-world-new-ai-chip/" target="_blank">https://www.wired.com/2017/05/google-rattles-tech-world-new-ai-chip/
</a>
Here are some links related to ML & AI

Nice 2-y old tutorial with pictures:
 - <a href="http://www.iro.umontreal.ca/%7Ebengioy/talks/DL-Tutorial-NIPS2015.pdf" target="_blank">http://www.iro.umontreal.ca/%7Ebengioy/talks/DL-Tutorial-NIPS2015.pdf</a>

New online publication:
 - <a href="http://distill.pub" target="_blank">http://distill.pub</a>

Christopher Olah has a great blog with
very clear explanations of DL concepts
 - <a href="http://colah.github.io/" target="_blank">http://colah.github.io/</a>
 - <a href="https://github.com/colah/" target="_blank">https://github.com/colah/</a>

Nice short online book about DL:
 - <a href="http://neuralnetworksanddeeplearning.com/index.html" target="_blank">http://neuralnetworksanddeeplearning.com/index.html
</a>
Good 1-hr lecture by Yann LeCun:
 - <a href="https://www.youtube.com/watch?v=IbjF5VjniVE" target="_blank">https://www.youtube.com/watch?v=IbjF5VjniVE</a>

Stanford - 15 lectures ( CS231n ) Fei-Fei Li & Andrej Karpathy & Justin Johnson
 - <a href="https://www.youtube.com/channel/UC2__PIf36huAgKFumlOIs6A" target="_blank">https://www.youtube.com/channel/UC2__PIf36huAgKFumlOIs6A
</a>
Oxford - Deep Learning lectures - Nando de Freitas
 - <a href="https://www.youtube.com/user/ProfNandoDF/videos" target="_blank">https://www.youtube.com/user/ProfNandoDF/videos</a>

Hinton lectures (Neural Networks for Machine Learning)
 - <a href="https://www.youtube.com/user/colinmcd94/videos" target="_blank">https://www.youtube.com/user/colinmcd94/videos</a>

Ian Goodfellow PhD Defense Presentation
 - <a href="https://www.youtube.com/watch?v=ckoD_bE8Bhs" target="_blank">https://www.youtube.com/watch?v=ckoD_bE8Bhs</a>

GANs - short 5min video by Siraj Raval (he has lots of videos)
 - <a href="https://www.youtube.com/watch?v=deyOX6Mt_As" target="_blank">https://www.youtube.com/watch?v=deyOX6Mt_As</a>

The Great A.I. Awakening - by Gideon Lewis-Kraus, Dec. 14, 2016
 - <a href="https://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html" target="_blank">https://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html</a>

Google Translate research paper (Sep 2016)
 - <a href="https://arxiv.org/pdf/1609.08144v2.pdf" target="_blank">https://arxiv.org/pdf/1609.08144v2.pdf</a>

Andrew Ng: Artificial Intelligence is the New Electricity
 - <a href="https://www.youtube.com/watch?v=21EiKfQYZXc" target="_blank">https://www.youtube.com/watch?v=21EiKfQYZXc</a>

Andrew Ng - The State of Artificial Intelligence (Dec 15, 2017)
 - <a href="https://www.youtube.com/watch?v=NKpuX_yzdYs" target="_blank">https://www.youtube.com/watch?v=NKpuX_yzdYs</a>

Follow on Facebook:
 - Yann LeCun
 - Adversarial Training
 - Deep AI
 - Deep Learning Patterns, Methodology and Strategy
 - Montreal.AI
 - &hellip;

Newsletters, meetups, courses:
 - <a href="https://opendatascience.com/" target="_blank">https://opendatascience.com/</a>
 - <a href="http://DataScienceWeekly.org" target="_blank">http://DataScienceWeekly.org</a>
 - NYC-Machine-Learning-list@meetup.com
 - NYC Artificial Intelligence & Deep Learning@meetup.com
 - <a href="http://machinelearningmastery.com" target="_blank">http://machinelearningmastery.com</a>
 - <a href="http://byteacademy.co/all-courses/data-science-mini-courses/" target="_blank">http://byteacademy.co/all-courses/data-science-mini-courses/</a>
 - <a href="https://www.coursera.org/courses/?languages=en&query=deep+learning" target="_blank">https://www.coursera.org/courses/?languages=en&query=deep+learning</a>
 - <a href="https://www.udacity.com/course/deep-learning--ud730" target="_blank">https://www.udacity.com/course/deep-learning--ud730</a>
 - <a href="https://www.youtube.com/watch?v=MYp3OwkiJAs" target="_blank">https://www.youtube.com/watch?v=MYp3OwkiJAs</a> - (in Russian)
 - ...

TensorFlow is an open source software library
Written by Google Brain Team (C++, Python)
Available for Linux, Mac, Windows

Deep Learning Frameworks Compared (5:10):
 - <a href="https://www.youtube.com/watch?v=MDP9FfsNx60" target="_blank">https://www.youtube.com/watch?v=MDP9FfsNx60</a>
TFLearn &ndash; a beginners wrapper around TensorFlow:
 - <a href="http://tflearn.org" target="_blank">http://tflearn.org</a>
</pre>
</html>
